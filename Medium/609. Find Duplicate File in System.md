# 609. Find Duplicate File in System — Architectural HashMap Grouping

## Problem Summary

We are given a list of directory info strings.

Each string has format:  
`"root/d1/.../dm f1.txt(content1) f2.txt(content2) ... fn.txt(contentN)"`

**Goal**: group all file paths that have **identical content**.

Return a list of groups, each containing **≥2 duplicate file paths**.

---

## Core Idea

**Files are identified by path, but duplicates are defined by content.**

**Strategy**:

- Parse each string
- Extract **content** as key
- Map `content → list of full file paths`
- After processing, collect groups with **size ≥ 2**

This is a **classic parsing + hashmap grouping** problem.

---

## Full Implementation (C#)

```csharp
public class Solution 
{
    public IList<IList<string>> FindDuplicate(string[] paths) 
    {
        Dictionary<string, List<string>> contentMap = new Dictionary<string, List<string>>();
        
        foreach (string entry in paths) 
        {
            string[] parts = entry.Split(' ');
            string dir = parts[0];
            
            for (int i = 1; i < parts.Length; i++) 
            {
                string file = parts[i];
                int leftParen = file.IndexOf('(');
                int rightParen = file.IndexOf(')');
                
                string fileName = file.Substring(0, leftParen);
                string content = file.Substring(leftParen + 1, rightParen - leftParen - 1);
                
                string fullPath = dir + "/" + fileName;
                
                if (!contentMap.ContainsKey(content))
                    contentMap[content] = new List<string>();
                    
                contentMap[content].Add(fullPath);
            }
        }
        
        List<IList<string>> result = new List<IList<string>>();
        foreach (var kv in contentMap) 
        {
            if (kv.Value.Count > 1)
                result.Add(kv.Value);
        }
        
        return result;
    }
}
```

## Complexity

- **Time**: **O(total length of input)**  
  Each file string parsed once.
- **Space**: **O(number of files)**  
  Dictionary stores file paths grouped by content.

## Pitfalls

- Must exclude files with **unique content** (only ≥2 duplicates matter).
- Careful parsing of `filename(content)` format.
- Large input size → efficient string operations required.

## Edge Cases

- Only one file → result = empty list
- All files unique → result = empty list
- All files same content → one group with all paths

## Sanity Checks

- Input:  
  `["root/a 1.txt(abcd) 2.txt(efgh)","root/c 3.txt(abcd)","root/c/d 4.txt(efgh)","root 4.txt(efgh)"]`  
  → Output:  
  `[["root/a/2.txt","root/c/d/4.txt","root/4.txt"],["root/a/1.txt","root/c/3.txt"]]`

- Input:  
  `["root/a 1.txt(abcd) 2.txt(efgh)","root/c 3.txt(abcd)","root/c/d 4.txt(efgh)"]`  
  → Output:  
  `[["root/a/2.txt","root/c/d/4.txt"],["root/a/1.txt","root/c/3.txt"]]`

## Key Takeaway

This is a **parsing + hashmap grouping** problem:

- Parse directory info strings
- Map **content → file paths**
- Collect groups with **≥2 duplicates**
**Clean O(n) solution with dictionary.**

---
